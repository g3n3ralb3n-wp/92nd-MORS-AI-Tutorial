{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence and Data Science in Operations Research\n",
    "## Machine Learning Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_svm(X_train, y_train, X_test, y_test, \n",
    "              C=1.0, kernel='rbf', degree=3, gamma='scale', \n",
    "              coef0=0.0, shrinking=True, probability=False, \n",
    "              tol=1e-3, cache_size=200, class_weight=None, \n",
    "              verbose=False, max_iter=-1, decision_function_shape='ovr', \n",
    "              break_ties=False, random_state=None):\n",
    "    \"\"\"\n",
    "    Trains an SVM model using the provided training data and evaluates it on the test data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature set\n",
    "    - y_train: Training labels\n",
    "    - X_test: Test feature set\n",
    "    - y_test: Test labels\n",
    "    - C: Regularization parameter (default=1.0)\n",
    "    - kernel: Specifies the kernel type to be used in the algorithm (default='rbf')\n",
    "    - degree: Degree of the polynomial kernel function (default=3)\n",
    "    - gamma: Kernel coefficient (default='scale')\n",
    "    - coef0: Independent term in kernel function (default=0.0)\n",
    "    - shrinking: Whether to use the shrinking heuristic (default=True)\n",
    "    - probability: Whether to enable probability estimates (default=False)\n",
    "    - tol: Tolerance for stopping criterion (default=1e-3)\n",
    "    - cache_size: Size of the kernel cache (default=200)\n",
    "    - class_weight: Set the parameter C of class i to class_weight[i]*C (default=None)\n",
    "    - verbose: Enable verbose output (default=False)\n",
    "    - max_iter: Hard limit on iterations within solver (default=-1)\n",
    "    - decision_function_shape: Whether to return a one-vs-rest ('ovr') decision function or not ('ovo') (default='ovr')\n",
    "    - break_ties: If true, decision_function_shape='ovr' and number of classes > 2, predict will break ties (default=False)\n",
    "    - random_state: Controls the pseudo random number generation for shuffling the data (default=None)\n",
    "    \n",
    "    Returns:\n",
    "    - model: The trained SVM model\n",
    "    - test_score: The accuracy score on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the SVM model with the provided hyperparameters\n",
    "    model = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma, coef0=coef0, \n",
    "                shrinking=shrinking, probability=probability, tol=tol, \n",
    "                cache_size=cache_size, class_weight=class_weight, verbose=verbose, \n",
    "                max_iter=max_iter, decision_function_shape=decision_function_shape, \n",
    "                break_ties=break_ties, random_state=random_state)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy score\n",
    "    test_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return model, test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_logistic_regression(X_train, y_train, X_test, y_test, \n",
    "                              penalty='l2', dual=False, tol=1e-4, C=1.0, \n",
    "                              fit_intercept=True, intercept_scaling=1, \n",
    "                              class_weight=None, random_state=None, \n",
    "                              solver='lbfgs', max_iter=100, multi_class='auto', \n",
    "                              verbose=0, warm_start=False, n_jobs=None, l1_ratio=None):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model using the provided training data and evaluates it on the test data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature set\n",
    "    - y_train: Training labels\n",
    "    - X_test: Test feature set\n",
    "    - y_test: Test labels\n",
    "    - penalty: Used to specify the norm used in the penalization (default='l2')\n",
    "    - dual: Dual or primal formulation (default=False)\n",
    "    - tol: Tolerance for stopping criteria (default=1e-4)\n",
    "    - C: Inverse of regularization strength (default=1.0)\n",
    "    - fit_intercept: Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function (default=True)\n",
    "    - intercept_scaling: Useful only when the solver ‘liblinear’ is used (default=1)\n",
    "    - class_weight: Weights associated with classes (default=None)\n",
    "    - random_state: Controls the random number generation for shuffling the data (default=None)\n",
    "    - solver: Algorithm to use in the optimization problem (default='lbfgs')\n",
    "    - max_iter: Maximum number of iterations taken for the solvers to converge (default=100)\n",
    "    - multi_class: If the option chosen is ‘ovr’, then a binary problem is fit for each label (default='auto')\n",
    "    - verbose: For the liblinear and lbfgs solvers set verbose to any positive number for verbosity (default=0)\n",
    "    - warm_start: Reuse the solution of the previous call to fit as initialization (default=False)\n",
    "    - n_jobs: Number of CPU cores used when parallelizing over classes if multi_class='ovr' (default=None)\n",
    "    - l1_ratio: The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1 (default=None)\n",
    "    \n",
    "    Returns:\n",
    "    - model: The trained Logistic Regression model\n",
    "    - test_score: The accuracy score on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Logistic Regression model with the provided hyperparameters\n",
    "    model = LogisticRegression(penalty=penalty, dual=dual, tol=tol, C=C, \n",
    "                               fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, \n",
    "                               class_weight=class_weight, random_state=random_state, solver=solver, \n",
    "                               max_iter=max_iter, multi_class=multi_class, verbose=verbose, \n",
    "                               warm_start=warm_start, n_jobs=n_jobs, l1_ratio=l1_ratio)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy score\n",
    "    test_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return model, test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_decision_tree(X_train, y_train, X_test, y_test, \n",
    "                        criterion='gini', splitter='best', max_depth=None, \n",
    "                        min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                        max_features=None, random_state=None, max_leaf_nodes=None, \n",
    "                        min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0):\n",
    "    \"\"\"\n",
    "    Trains a Decision Tree model using the provided training data and evaluates it on the test data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature set\n",
    "    - y_train: Training labels\n",
    "    - X_test: Test feature set\n",
    "    - y_test: Test labels\n",
    "    - criterion: The function to measure the quality of a split (default='gini')\n",
    "    - splitter: The strategy used to choose the split at each node (default='best')\n",
    "    - max_depth: The maximum depth of the tree (default=None)\n",
    "    - min_samples_split: The minimum number of samples required to split an internal node (default=2)\n",
    "    - min_samples_leaf: The minimum number of samples required to be at a leaf node (default=1)\n",
    "    - min_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights required to be at a leaf node (default=0.0)\n",
    "    - max_features: The number of features to consider when looking for the best split (default=None)\n",
    "    - random_state: Controls the randomness of the estimator (default=None)\n",
    "    - max_leaf_nodes: Grow a tree with max_leaf_nodes in best-first fashion (default=None)\n",
    "    - min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value (default=0.0)\n",
    "    - class_weight: Weights associated with classes (default=None)\n",
    "    - ccp_alpha: Complexity parameter used for Minimal Cost-Complexity Pruning (default=0.0)\n",
    "    \n",
    "    Returns:\n",
    "    - model: The trained Decision Tree model\n",
    "    - test_score: The accuracy score on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Decision Tree model with the provided hyperparameters\n",
    "    model = DecisionTreeClassifier(criterion=criterion, splitter=splitter, max_depth=max_depth, \n",
    "                                   min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \n",
    "                                   min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features, \n",
    "                                   random_state=random_state, max_leaf_nodes=max_leaf_nodes, \n",
    "                                   min_impurity_decrease=min_impurity_decrease, class_weight=class_weight, \n",
    "                                   ccp_alpha=ccp_alpha)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy score\n",
    "    test_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return model, test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test, y_test, \n",
    "                        n_estimators=100, criterion='gini', max_depth=None, \n",
    "                        min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                        max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                        bootstrap=True, oob_score=False, n_jobs=None, random_state=None, \n",
    "                        verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, \n",
    "                        max_samples=None):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model using the provided training data and evaluates it on the test data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training feature set\n",
    "    - y_train: Training labels\n",
    "    - X_test: Test feature set\n",
    "    - y_test: Test labels\n",
    "    - n_estimators: The number of trees in the forest (default=100)\n",
    "    - criterion: The function to measure the quality of a split (default='gini')\n",
    "    - max_depth: The maximum depth of the tree (default=None)\n",
    "    - min_samples_split: The minimum number of samples required to split an internal node (default=2)\n",
    "    - min_samples_leaf: The minimum number of samples required to be at a leaf node (default=1)\n",
    "    - min_weight_fraction_leaf: The minimum weighted fraction of the sum total of weights required to be at a leaf node (default=0.0)\n",
    "    - max_features: The number of features to consider when looking for the best split (default='auto')\n",
    "    - max_leaf_nodes: Grow trees with max_leaf_nodes in best-first fashion (default=None)\n",
    "    - min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value (default=0.0)\n",
    "    - bootstrap: Whether bootstrap samples are used when building trees (default=True)\n",
    "    - oob_score: Whether to use out-of-bag samples to estimate the generalization accuracy (default=False)\n",
    "    - n_jobs: The number of jobs to run in parallel (default=None)\n",
    "    - random_state: Controls the randomness of the estimator (default=None)\n",
    "    - verbose: Controls the verbosity when fitting and predicting (default=0)\n",
    "    - warm_start: When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble (default=False)\n",
    "    - class_weight: Weights associated with classes (default=None)\n",
    "    - ccp_alpha: Complexity parameter used for Minimal Cost-Complexity Pruning (default=0.0)\n",
    "    - max_samples: If bootstrap is True, the number of samples to draw from X to train each base estimator (default=None)\n",
    "    \n",
    "    Returns:\n",
    "    - model: The trained Random Forest model\n",
    "    - test_score: The accuracy score on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Random Forest model with the provided hyperparameters\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, \n",
    "                                   min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, \n",
    "                                   min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features, \n",
    "                                   max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=min_impurity_decrease, \n",
    "                                   bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n",
    "                                   verbose=verbose, warm_start=warm_start, class_weight=class_weight, \n",
    "                                   ccp_alpha=ccp_alpha, max_samples=max_samples)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the accuracy score\n",
    "    test_score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return model, test_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (120, 4)\n",
      "y_train shape: (120,)\n",
      "X_test shape: (30, 4)\n",
      "y_test shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model, score = train_logistic_regression(X_train, y_train, X_test, y_test, solver='liblinear', max_iter=200)\n",
    "print(f\"Test Accuracy: {score}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
