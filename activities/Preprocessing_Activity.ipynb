{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence and Data Science in Operations Research\n",
    "## Preprocssing Data Activity\n",
    "\n",
    "For this activity, you will use the predefined methods to preprocess your data. Ensure that you are considering the combination of methods and justify your selection of methods as well as your approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def min_max_normalization(data, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Applies Min-Max normalization to the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The dataset to be normalized.\n",
    "    feature_range (tuple): Desired range of transformed data.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Normalized dataset.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "    return normalized_data\n",
    "\n",
    "# Example usage:\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "normalized_data = min_max_normalization(data, feature_range=(0, 1))\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def standardize_data(data):\n",
    "    \"\"\"\n",
    "    Applies standardization (Z-score normalization) to the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The dataset to be standardized.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Standardized dataset.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(data)\n",
    "    return standardized_data\n",
    "\n",
    "# Example usage:\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "standardized_data = standardize_data(data)\n",
    "print(standardized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "import numpy as np\n",
    "\n",
    "def binarize_data(data, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Binarizes the dataset based on a threshold.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The dataset to be binarized.\n",
    "    threshold (float): The threshold value to binarize data.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Binarized dataset.\n",
    "    \"\"\"\n",
    "    binarizer = Binarizer(threshold=threshold)\n",
    "    binarized_data = binarizer.fit_transform(data)\n",
    "    return binarized_data\n",
    "\n",
    "# Example usage:\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "binarized_data = binarize_data(data, threshold=3)\n",
    "print(binarized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  2.  1.  2.  4.]\n",
      " [ 1.  3.  4.  9. 12. 16.]\n",
      " [ 1.  5.  6. 25. 30. 36.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "def add_polynomial_features(data, degree=2):\n",
    "    \"\"\"\n",
    "    Generates polynomial features of the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The dataset to be transformed.\n",
    "    degree (int): The degree of the polynomial features.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Dataset with polynomial features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    poly_features = poly.fit_transform(data)\n",
    "    return poly_features\n",
    "\n",
    "# Example usage:\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "poly_features = add_polynomial_features(data, degree=2)\n",
    "print(poly_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_labels(labels):\n",
    "    \"\"\"\n",
    "    Encodes categorical labels with values between 0 and n_classes-1.\n",
    "    \n",
    "    Parameters:\n",
    "    labels (array-like): The labels to be encoded.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Encoded labels.\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_labels = encoder.fit_transform(labels)\n",
    "    return encoded_labels\n",
    "\n",
    "# Example usage:\n",
    "labels = ['cat', 'dog', 'fish', 'dog', 'cat']\n",
    "encoded_labels = encode_labels(labels)\n",
    "print(encoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def apply_pca(data, n_components=None):\n",
    "    \"\"\"\n",
    "    Applies Principal Component Analysis (PCA) to the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The dataset to be transformed.\n",
    "    n_components (int, float, None or str): Number of components to keep.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Transformed dataset with principal components.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    return pca_data\n",
    "\n",
    "# Example usage:\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "pca_data = apply_pca(data, n_components=2)\n",
    "print(pca_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Ranking (Fisher's Linear Discriminant): [10  1  9  2  7  8  5  6  3  4]\n",
      "Fisher Scores for each feature: [0.27896741 0.19563216 0.02062529 0.0104751  0.04265937 0.04115039\n",
      " 0.14824801 0.109263   0.25293161 0.61988189]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Fisher's Linear Discriminant function\n",
    "def fisher_score(data, target):\n",
    "    \"\"\"\n",
    "    Computes Fisher score for each feature.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The dataset with features.\n",
    "    target (array-like): The target variable.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Fisher scores for each feature.\n",
    "    \"\"\"\n",
    "    unique_classes = np.unique(target)\n",
    "    mean_overall = np.mean(data, axis=0)\n",
    "    \n",
    "    numerator = np.zeros(data.shape[1])\n",
    "    denominator = np.zeros(data.shape[1])\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        cls_data = data[target == cls]\n",
    "        mean_cls = np.mean(cls_data, axis=0)\n",
    "        numerator += cls_data.shape[0] * (mean_cls - mean_overall) ** 2\n",
    "        denominator += np.sum((cls_data - mean_cls) ** 2, axis=0)\n",
    "    \n",
    "    fisher_scores = numerator / denominator\n",
    "    return fisher_scores\n",
    "\n",
    "def rank_features_fisher(data, target):\n",
    "    \"\"\"\n",
    "    Ranks features using Fisher's Linear Discriminant.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The dataset with features.\n",
    "    target (array-like): The target variable.\n",
    "    \n",
    "    Returns:\n",
    "    array-like: Ranking of features.\n",
    "    \"\"\"\n",
    "    scores = fisher_score(data, target)\n",
    "    ranking = np.argsort(scores)[::-1] + 1\n",
    "    return ranking\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "data, target = make_classification(n_samples=100, n_features=10, n_informative=5, n_redundant=2, n_classes=3, random_state=42)\n",
    "\n",
    "# Apply Fisher's Linear Discriminant feature ranking\n",
    "feature_ranking_fisher = rank_features_fisher(data, target)\n",
    "\n",
    "print(\"Feature Ranking (Fisher's Linear Discriminant):\", feature_ranking_fisher)\n",
    "\n",
    "# Display the Fisher scores for each feature\n",
    "fisher_scores = fisher_score(data, target)\n",
    "print(\"Fisher Scores for each feature:\", fisher_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2]\n",
      " [ 3  4]\n",
      " [ 5  6]\n",
      " [ 7  8]\n",
      " [ 9 10]\n",
      " [ 7  8]]\n",
      "[0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simple_oversample_data(data, target):\n",
    "    \"\"\"\n",
    "    Applies simple random oversampling to the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The dataset with features.\n",
    "    target (array-like): The target variable.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Oversampled dataset and target.\n",
    "    \"\"\"\n",
    "    # Separate minority and majority classes\n",
    "    unique_classes, class_counts = np.unique(target, return_counts=True)\n",
    "    minority_class = unique_classes[np.argmin(class_counts)]\n",
    "    majority_class = unique_classes[np.argmax(class_counts)]\n",
    "\n",
    "    # Indices of minority and majority class samples\n",
    "    minority_indices = np.where(target == minority_class)[0]\n",
    "    majority_indices = np.where(target == majority_class)[0]\n",
    "\n",
    "    # Number of samples to match majority class\n",
    "    num_to_oversample = len(majority_indices) - len(minority_indices)\n",
    "\n",
    "    # Randomly replicate samples from the minority class\n",
    "    oversampled_indices = np.random.choice(minority_indices, num_to_oversample, replace=True)\n",
    "\n",
    "    # Combine original data with oversampled minority class samples\n",
    "    oversampled_data = np.concatenate([data, data[oversampled_indices]])\n",
    "    oversampled_target = np.concatenate([target, target[oversampled_indices]])\n",
    "\n",
    "    return oversampled_data, oversampled_target\n",
    "\n",
    "# Example usage:\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "target = np.array([0, 1, 0, 1, 0])\n",
    "oversampled_data, oversampled_target = simple_oversample_data(data, target)\n",
    "print(oversampled_data)\n",
    "print(oversampled_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, use these on the below sampled dataset from the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Combine the data and target into a single DataFrame for easy manipulation\n",
    "iris_df = pd.DataFrame(data, columns=iris.feature_names)\n",
    "iris_df['target'] = target\n",
    "\n",
    "# Randomly sample 50% of the observations\n",
    "iris_df_sampled = iris_df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate the features and the target\n",
    "data_sampled = iris_df_sampled.drop(columns=['target']).values\n",
    "target_sampled = iris_df_sampled['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
